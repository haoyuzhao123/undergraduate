\documentclass{article}

\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}

\title{\textbf{Computational Biology: Project 2 Report}}

\author[1]{Haoyu Zhao 2016012390;
  Lukai Li 2015012202;
  Yi Dai 2015012204}
\date{}

\begin{document}
\maketitle
\clearpage

\section{Introduction}
\textbf{Acknowledgement:} In this project, Haoyu Zhao came up with an idea that slightly change the k-means clustering algorithm and apply it into this project. He doesn't know whether this method has already been published and he doesn't know whether this small change can improve the performance or get more things useful in this project. He will present the result in this report can try to illustrate the change. He will also discuss the method in the class presentation and discuss whether the change does some meaningful things.\par

\section{Flow Chart of Algorithm}

\section{Details of Algorithm}
\subsection{CTF Correction and Down Sampling}

First we do the CTF correction for each of the graph. The parameters can be obtained from the .star file and the CTF function is
\begin{equation*}
	CTF(s,\theta) = -\left(\sqrt{1 - Q^2}sin\gamma(s,\theta) + Qcos\gamma(s,\theta)\right).
\end{equation*}
where
\begin{equation*}
	\gamma(s,\theta) = 2\pi \left(-\frac{C_s\lambda^3s^4}{4} + \frac{\triangle z(\theta)\lambda s^2}{2}\right)
\end{equation*}
and
\begin{equation*}
	\triangle z(\theta) = \triangle z_0 + \triangle z_1cos(2(\theta - \theta_0)).
\end{equation*}
~\\
\par
We use the phase flipping to do the ctf correction. We first do the 2-dimentional Fast Fourier Transformation to the original graph, and then divide the 2dfft transformed graph by the absolute value of the ctf correction function. Then we use the inverse transformation of the 2-d fast fourier transformation to get the ctf corrected graph.\par
Then because of the computational resources, we do the down sampling. We first select the cells out by an 160 by 160 pixels image, and we do the mean pooling to each 2 by 2 block and we finally get the 80 by 80 cell images. We rescale the image by a constant in order to see the distance result clearly in the measurement.
\subsection{First Clustering}
	We first use the k-means algorithm to do the clustering. We set the nubmer of the initial number of clusters to 25 and we eleminate the clusters that have less then 100(or 75) to avoid from some outliers. Then we add the image in the same class up and get the center of each of the clusters. We save the images and then save the centers of the clustering 
\subsection{Second Clustering}
	From the first clustering we get the centers of each clusters. We find that after the first clustering, many clustering are actually the same. The only different is that the images rotate for some angle. And to let each of the clusters has enough images, we cannot enlarge the initial number of the clusters. So what we do is just eleminate the features that clustered from the first clustering and just do another clustering.\par
	In the elemination of the clustering, we first try to eleminate each of the images just by its class, that is for $x^{(i)}$ that have the center $\mu^{(i)}$, we change
	\begin{equation*}
		x^{(i)} \leftarrow x^{(i)} - \frac{x^{(i)}}{\mid x^{(i)}\mid^2}\cdot x^{(i)T}\mu^{(i)}
	\end{equation*}
and then use the kmeans clustering to do the second cluster. But after do some experiment, we found that the dimension of the image is too high and this cannot do some real change, so we change eleminating the center of clustering of each image to eleminating the subspace generated by all of the centers. Suppose that the matrix of the images is
\begin{equation*}
	X = \left[
	\begin{aligned}
		-&x^{(1)T}-\\
		-&x^{(2)T}-\\
		-&x^{(3)T}-\\
		-&\dots-\\
		-&x^{(n)T}-
	\end{aligned}\right]
\end{equation*}
and the centers matrix is
\begin{equation*}
	\Sigma = \left[
	\begin{aligned}
	-&\mu^{(1)T}-\\
	-&\mu^{(2)T}-\\
	-&\mu^{(3)T}-\\
	-&\dots-\\
	-&\mu^{(n)T}-
	\end{aligned}\right]
\end{equation*}
Then the new data matrix for clustering is
\begin{equation*}
	X_0 = X - (\Sigma^T(\Sigma\Sigma^T)^{-1}\Sigma X^T).
\end{equation*}
Then we do the second clustering using the eleminated subspaced data.\par
We can actually clustering using many ways of eleminating the subspaced data. We can eleminate each clustering center, we can eleminate the subspace generated by the last time clustering centers(we do this in the project), and we can eleminate the subspace generated by all of the previous clustering centers(we test this method).

\subsection{Measurement}
There are 2 ways to measure the algorithm and the performance.\par The first is to just look at the clustered images and see whether the images make sense.\par
The second is to compute the sum of the squared distance from each of the image to its centers. We know that it is just the cost function of the k-means algorithm. For those clusters that we eliminate, we do not calculate the squared distance of them. We first take the mean of the squared distance in each of the clusters and then average them for the number of the clusters.


\section{Final Result and Performance Evaluation}
In this section we will present some tests of different implementations. We will just present the images for all test and we will present the images and the mean of the squared distance of the final implementation.\par

These are the test images for the k-means algorithm, for initial clusters numbers are 16 and 36.
\includegraphics[scale=•]{•}
\par

These are the 


\section{Discussion}
First we discuss some of the shortcoming of our implementation or designing of our algorithm.
\begin{enumerate}
\item[1] We think that there may be something wrong with our CTF correction function because the corrected graph isn't change a lot compared with the origianl graph. 
\item[2] We do the down sampling in the image processing step because we do not have enough computational resources. We think that although this step make us easier to do the computation but this will actually lose some mini-structure of the protein.
\item[3] The dimension of each image is 6400(80 by 80) and we just have about 6000-7000 images. The dimension of the image compared with the number of the images is very high. So we should do the PCA or use the autoencoder to reduce the dimension of the data and by the way decrease the noise to some extent, but considering doing the PCA need to generate a 6400 by 6400 matrix and we think that our computer cannot afford this, so we just skip this step in the project.
\end{enumerate}

Then we discuss the contradictory part of our algorithm, that clustering several times after eleminating the subspace of the centers in the previous clustering, theses are our thoughts:
\begin{enumerate}
\item[1] We don't know whether this process make sense or can generate some meaningful images, we just come up with this idea and tried in this project. We just think that it is okey in the low dimension space with very simple data. This method lacks the experiment and theory proof.
\item[2] After the eleminating 
\end{enumerate}

\section{Contribution of Each Member}

\section{References}


\end{document}