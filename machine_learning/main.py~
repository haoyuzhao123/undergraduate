# -*- coding: utf-8 -*-
"""
Created on Wed Jan  3 22:57:03 2018

@author: zhy
"""

import tensorflow as tf
import numpy as np

from const import Const

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot = True)


#functions for initializing the parameters
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev = 0.1)
    return tf.Variable(initial)
    
def bias_variable(shape):
    initial = tf.constant(0.1, shape = shape)
    return tf.Variable(initial)
    
def squash(vector):
    '''Squashing function corresponding to Eq. 1
    '''
    vec_squared_norm = tf.reduce_sum(tf.square(vector), -2, keep_dims=True)
    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + Const.epsilon)
    vec_squashed = scalar_factor * vector  # element-wise
    return(vec_squashed)

def routing(vec, b_IJ):
    ''' The routing algorithm.
     '''

    W = tf.get_variable('Weight', shape=(1, Const.dcaps_size, 10, Const.length_pcaps, \
                                         Const.length_dcaps), dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=0.1))

    # Eq.2, calc u_hat
    vec = tf.tile(vec, [1, 1, 10, 1, 1])
    W = tf.tile(W, [Const.batch_size, 1, 1, 1, 1])
    u_hat = tf.matmul(W, vec, transpose_a=True)
    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')
    
    iter_routing = 3

    # line 3,for r iterations do
    for r_iter in range(iter_routing):
        with tf.variable_scope('iter_' + str(r_iter)):
            c_IJ = tf.nn.softmax(b_IJ, dim=2)

            if r_iter == iter_routing - 1:
                s_J = tf.multiply(c_IJ, u_hat)
                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)

                # squash using Eq.1,
                v_J = squash(s_J)
            elif r_iter < iter_routing - 1:  # Inner iterations, do not apply backpropagation
                s_J = tf.multiply(c_IJ, u_hat_stopped)
                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)
                v_J = squash(s_J)

                v_J_tiled = tf.tile(v_J, [1, Const.dcaps_size, 1, 1, 1])
                u_produce_v = tf.matmul(u_hat_stopped, v_J_tiled, transpose_a=True)
                b_IJ += u_produce_v

    return(v_J)

def main():
    X = tf.placeholder(tf.float32, shape = [None, 784])
    Y_ = tf.placeholder(tf.float32, shape = [None, 10])
    
    X_image = tf.reshape(X, [-1,28,28,1])
    
    h_conv1 = tf.contrib.layers.conv2d(X_image, num_outputs=Const.ker_output_conv, kernel_size = Const.ker_size_conv, stride = Const.ker_strides_conv, padding = 'VALID')

    primarycaps = tf.contrib.layers.conv2d(h_conv1, kernel_size = Const.ker_size_pcaps, num_outputs = Const.ker_output_pcaps * Const.length_pcaps, stride = Const.ker_strides_pcaps, padding = 'VALID')

    tf.reshape(primarycaps, [Const.batch_size,-1,Const.length_pcaps,1])
    primarycaps = squash(primarycaps)
    
    #the digitcaps
    primarycaps = tf.reshape(primarycaps, [Const.batch_size,-1,1,Const.length_pcaps,1])
        
    #assume that the size of the strides matches
    conv_len = 28 - Const.ker_size_conv + 1
    assert(conv_len % Const.ker_strides_conv == 0)
    conv_len = conv_len / Const.ker_strides_conv
    
    pcaps_len = conv_len - Const.ker_size_pcaps + 1
    assert(pcaps_len % Const.ker_strides_pcaps == 0)
    pcaps_len = pcaps_len / Const.ker_strides_pcaps
    
    size_of_dcaps = Const.ker_output_pcaps * pcaps_len ** 2
    assert(size_of_dcaps == Const.dcaps_size)
    #10 is the number of output here
    b_IJ = tf.constant(np.zeros([Const.batch_size, Const.dcaps_size, 10, 1, 1], dtype=np.float32))
    
    ## edit here
    digitcaps = routing(primarycaps, b_IJ)
    digitcaps = tf.squeeze(digitcaps, axis=1)
    
    #masking
    # a). calc ||v_c||, then do softmax(||v_c||)
    # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]
    v_length = tf.sqrt(tf.reduce_sum(tf.square(digitcaps),
                                          axis=2, keep_dims=True) + Const.epsilon)
    softmax_v = tf.nn.softmax(v_length, dim=1)
    assert softmax_v.get_shape() == [Const.batch_size, 10, 1, 1]

    # b). pick out the index of max softmax val of the 10 caps
    # [batch_size, 10, 1, 1] => [batch_size] (index)
    argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))
    assert argmax_idx.get_shape() == [Const.batch_size, 1, 1]
    argmax_idx = tf.reshape(argmax_idx, shape=(Const.batch_size, ))
    
    # self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)
    masked_v = tf.multiply(tf.squeeze(digitcaps), tf.reshape(Y_, (-1, 10, 1)))
    v_length = tf.sqrt(tf.reduce_sum(tf.square(digitcaps), axis=2, keep_dims=True) + Const.epsilon)
    
    #reconstruction
    vector_j = tf.reshape(masked_v, shape=(Const.batch_size, -1))
    fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=Const.size_fc1)
    fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=Const.size_fc2)
    decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)
    
    #compute the loss
    # 1. The margin loss

    # [batch_size, 10, 1, 1]
    # max_l = max(0, m_plus-||v_c||)^2
    max_l = tf.square(tf.maximum(0., Const.m_plus - v_length))
    # max_r = max(0, ||v_c||-m_minus)^2
    max_r = tf.square(tf.maximum(0., v_length - Const.m_minus))
    assert max_l.get_shape() == [Const.batch_size, 10, 1, 1]

    # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]
    max_l = tf.reshape(max_l, shape=(Const.batch_size, -1))
    max_r = tf.reshape(max_r, shape=(Const.batch_size, -1))

    # calc T_c: [batch_size, 10]
    # T_c = Y, is my understanding correct? Try it.
    T_c = Y_
    # [batch_size, 10], element-wise multiply
    L_c = T_c * max_l + Const.lambda_val * (1 - T_c) * max_r

    margin_loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))

    # 2. The reconstruction loss
    orgin = tf.reshape(X, shape=(Const.batch_size, -1))
    squared = tf.square(decoded - orgin)
    reconstruction_err = tf.reduce_mean(squared)

    # 3. Total loss
    # The paper uses sum of squared error as reconstruction error, but we
    # have used reduce_mean in `# 2 The reconstruction loss` to calculate
    # mean squared error. In order to keep in line with the paper,the
    # regularization scale should be 0.0005*784=0.392
    total_loss = margin_loss + Const.regularization_scale * reconstruction_err
    
    #prediction accuraccy
    correct_prediction = tf.equal(tf.cast(tf.argmax(Y_,1),tf.int32), argmax_idx)
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    
    #training
    step = tf.Variable(0, trainable = False)
    optimizer = tf.train.AdamOptimizer()
    train_op = optimizer.minimize(total_loss,global_step=step)
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        print('variable initialized.')
        print('begin training')
        for i in range(20000):
            batch = mnist.train.next_batch(Const.batch_size)
            if i % 10 == 0:
                train_accuracy = accuracy.eval(feed_dict = {X: batch[0], Y_: batch[1]})
                print('step %d, training accuracy %g' % (i, train_accuracy))
            train_op.run(feed_dict = {X: batch[0], Y_: batch[1]})
            
        
        #print('test accuracy %g' % accuracy.eval(
        #        feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))

        acc = np.zeros(100)
        for j in range(100):
            batch = mnist.test.next_batch(100)
            acc[j] = accuracy.eval(feed_dict={X: batch[0], Y_: batch[1]})
            print('test data batch %d accuracy: %g' % (j,acc[j]))
        print('test accuracy: %g' % (np.sum(acc)/100.0))
    
    
    
if __name__ == '__main__':
    main()
