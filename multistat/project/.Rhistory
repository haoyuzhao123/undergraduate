best_c = 0.0
Cs <- 1:30 / 10
sigmas <- 1:30 / 10
vec <- 1:100
random_vector <- sample(vec, 100)
for (c in Cs) {
for (sigma in sigmas) {
temp_rate = costJ(c,sigma,training_data_nnoise,training_labels_nnoise,random_vector)
if (temp_rate > max_rate) {
max_rate = temp_rate
best_sigma = sigma
best_c = c
}
print("pass")
}
}
for (c in Cs) {
for (sigma in sigmas) {
temp_rate = costJ(c,sigma,training_data_nnoise,training_labels_nnoise,random_vector)
if (temp_rate > max_rate) {
max_rate = temp_rate
best_sigma = sigma
best_c = c
}
}
}
svm_object = ksvm(training_data_nnoise,training_labels_nnoise, type = "C-svc",
kernel = "rbfdot", sigma = best_sigma, C = best_c)
preds = predict(svm_object, testing_data_nnoise)
print(sum(preds == testing_labels_nnoise) / 1000)
preds = predict(svm_object, testing_data_nnoise)
print(sum(preds == testing_labels_nnoise) / 2000)
max_rate = 0.0
best_sigma = 0.0
best_c = 0.0
Cs <- 1:30 / 10
sigmas <- 1:30 / 10
vec <- 1:100
random_vector <- sample(vec, 100)
for (c in Cs) {
for (sigma in sigmas) {
temp_rate = costJ(c,sigma,training_data_noise,training_labels_noise,random_vector)
if (temp_rate > max_rate) {
max_rate = temp_rate
best_sigma = sigma
best_c = c
}
}
}
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = best_sigma, C = best_c)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 1)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 0.1
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.2, C = 0.1
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.2, C = 0.1
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.3, C = 0.1
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.3, C = 0.1
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.3, C = 0.1)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.3, C = 0.1)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 0.1)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 0.5)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 1)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 1.3)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 1.4)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 1.5)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.2, C = 1.5)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.3, C = 1.5)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.4, C = 1.5)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.4, C = 1.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.5, C = 1.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.6, C = 1.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.5, C = 1.4)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.5, C = 1.6)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.5, C = 2.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.5, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.5, C = 4.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 4.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 4.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.2, C = 4.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.2, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.1, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 0.3, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 3, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 5, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 5, C = 10.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 5, C = 7.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 3, C = 7.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
svm_object = ksvm(training_data_noise,training_labels_noise, type = "C-svc",
kernel = "rbfdot", sigma = 3, C = 5.0)
preds = predict(svm_object, testing_data_noise)
print(sum(preds == testing_labels_noise) / 2000)
help("princomp")
l1
l2
l1
l1
l1
1
v
v
tx <- v * t(v)
t(v)
v * t(v)
v * v
v %*% t(v)
diag(5)
help("sum)
"\"
help("sum")
helo("factanal")
help("factanal")
install.packages('dplyr')
library(data.table)
install.packages('data.table')
setwd('~/codes/pycode/undergraduate/statlearn/')
## This is an R implementation of sample_dask.py. It uses the Marshall-Palmer relation
## to calculate hourly rain rates for each Id. It produces a submission file identical
## to the 'sample_solution.csv' file provided in the Data section of this competition
## to at least 9 decimal places. It earns a MAE score of 24.06968 on the leaderboard
## exactly equal to the sample solution benchmark. And it's faster ;)
## This may be useful in training, I'm just not sure exactly how yet.
## Someone better in R than me could probably clean up/refactor the code a bit.
library(dplyr)
library(data.table)
test <- fread('./test.csv')
mpalmer <- function(ref, minutes_past) {
# order reflectivity values and minutes_past
sort_min_index = order(minutes_past)
minutes_past <- minutes_past[sort_min_index]
ref <- ref[sort_min_index]
# calculate the length of time for which each reflectivity value is valid
valid_time <- rep(0, length(minutes_past))
valid_time[1] <- minutes_past[1]
if (length(valid_time) > 1) {
for (i in seq(2, length(minutes_past))) {
valid_time[i] <- minutes_past[i] - minutes_past[i-1]
}
valid_time[length(valid_time)] = valid_time[length(valid_time)] + 60 - sum(valid_time)
} else {
# if only 1 observation, make it valid for the entire hour
valid_time <- 60
}
valid_time = valid_time / 60
# calculate hourly rain rates using marshall-palmer weighted by valid times
sum <- 0
for (i in seq(length(ref))) {
if (!is.na(ref[i])) {
mmperhr <- 0.02730701 * 10 ^ (ref[i] * 0.02853282)
sum <- sum + mmperhr * valid_time[i]
}
}
return(sum)
}
results <- test %>% group_by(Id) %>% summarize(Expected=mpalmer(Ref, minutes_past))
write.csv(results, file='test_solution.csv', row.names=FALSE)
## This is an R implementation of sample_dask.py. It uses the Marshall-Palmer relation
## to calculate hourly rain rates for each Id. It produces a submission file identical
## to the 'sample_solution.csv' file provided in the Data section of this competition
## to at least 9 decimal places. It earns a MAE score of 24.06968 on the leaderboard
## exactly equal to the sample solution benchmark. And it's faster ;)
## This may be useful in training, I'm just not sure exactly how yet.
## Someone better in R than me could probably clean up/refactor the code a bit.
library(dplyr)
library(data.table)
test <- fread('./test.csv')
mpalmer <- function(ref, minutes_past) {
# order reflectivity values and minutes_past
sort_min_index = order(minutes_past)
minutes_past <- minutes_past[sort_min_index]
ref <- ref[sort_min_index]
# calculate the length of time for which each reflectivity value is valid
valid_time <- rep(0, length(minutes_past))
valid_time[1] <- minutes_past[1]
if (length(valid_time) > 1) {
for (i in seq(2, length(minutes_past))) {
valid_time[i] <- minutes_past[i] - minutes_past[i-1]
}
valid_time[length(valid_time)] = valid_time[length(valid_time)] + 60 - sum(valid_time)
} else {
# if only 1 observation, make it valid for the entire hour
valid_time <- 60
}
valid_time = valid_time / 60
# calculate hourly rain rates using marshall-palmer weighted by valid times
sum <- 0
for (i in seq(length(ref))) {
if (!is.na(ref[i])) {
mmperhr <- 0.02730701 * 10 ^ (ref[i] * 0.02853282)
sum <- sum + mmperhr * valid_time[i]
}
}
return(sum)
}
results <- test %>% group_by(Id) %>% summarize(Expected=mpalmer(Ref, minutes_past))
write.csv(results, file='test_solution.csv', row.names=FALSE)
## This is an R implementation of sample_dask.py. It uses the Marshall-Palmer relation
## to calculate hourly rain rates for each Id. It produces a submission file identical
## to the 'sample_solution.csv' file provided in the Data section of this competition
## to at least 9 decimal places. It earns a MAE score of 24.06968 on the leaderboard
## exactly equal to the sample solution benchmark. And it's faster ;)
## This may be useful in training, I'm just not sure exactly how yet.
## Someone better in R than me could probably clean up/refactor the code a bit.
library(dplyr)
library(data.table)
test <- fread('./test.csv')
mpalmer <- function(ref, minutes_past) {
# order reflectivity values and minutes_past
sort_min_index = order(minutes_past)
minutes_past <- minutes_past[sort_min_index]
ref <- ref[sort_min_index]
# calculate the length of time for which each reflectivity value is valid
valid_time <- rep(0, length(minutes_past))
valid_time[1] <- minutes_past[1]
if (length(valid_time) > 1) {
for (i in seq(2, length(minutes_past))) {
valid_time[i] <- minutes_past[i] - minutes_past[i-1]
}
valid_time[length(valid_time)] = valid_time[length(valid_time)] + 60 - sum(valid_time)
} else {
# if only 1 observation, make it valid for the entire hour
valid_time <- 60
}
# calculate hourly rain rates using marshall-palmer weighted by valid times
sum <- 0
for (i in seq(length(ref))) {
if (!is.na(ref[i])) {
mmperhr <- 0.02730701 * 10 ^ (ref[i] * 0.02853282)
sum <- sum + mmperhr * valid_time[i]
}
}
return(sum)
}
results <- test %>% group_by(Id) %>% summarize(Expected=mpalmer(Ref, minutes_past))
write.csv(results, file='test_solution.csv', row.names=FALSE)
dataread <- read.csv('./data4.csv')
str(dataread)
dataread <- read.csv('./data4.csv', header = F)
str(dataread)
new <- subset(dataread, V3 < 100)
write.csv(new)
write.csv(new, file = 'new.csv')
########################################################################
#----------------------begin of the project code------------------------
#----------------------load the packages--------------------------------
library(ggplot2)
library(corrplot)
#----------------------read the data from the data set------------------
raw_data <- read.csv('PRSA_data_2010.1.1-2014.12.31.csv', header = TRUE)
#there are some NA values in the data set, so I omit them
data_basic <- subset(raw_data, is.na(pm2.5)==FALSE)
setwd('~/Documents/u')
setwd('~/Documents/undergraduate/courses/freshman_2/multistat/project/')
library(ggplot2)
library(corrplot)
#----------------------read the data from the data set------------------
raw_data <- read.csv('PRSA_data_2010.1.1-2014.12.31.csv', header = TRUE)
data_basic <- subset(raw_data, is.na(pm2.5)==FALSE)
cormat <- cor(data_basic)
reg_var <- data_basic[,c(6,7,8,9,11,12,13)]
cormat <- cor(reg_var)
cormat
R12 <- cormat[1,2:7]
R21 <- cormat[2:7,1]
R22 <- cormat[2:7,2:7]
solve(R22)
newmat <- solve(R22) %*% R21 %*% R12
eigen(newmat)
t <- eigen(newmat)
t$values
cormat <- cor(reg_var)
R12 <- cormat[1,2:7]
R21 <- cormat[2:7,1]
R22 <- cormat[2:7,2:7]
R2 <- solve(R22) %*% R21 %*% R12
eigen(R2)
#-------then do some descriptive statistics to more than 1 variable---------
#-----------------the correlation of different variables--------------------
#------------delete the time and the wind direction variables---------------
reg_var <- data_basic[,c(6,7,8,9,11,12,13)]
corrplot(cor(reg_var), method='square')
cormat <- cor(reg_var)
R12 <- cormat[1,2:7]
R21 <- cormat[2:7,1]
R22 <- cormat[2:7,2:7]
R2 <- solve(R22) %*% R21 %*% R12
eigen(R2)
cor(reg_var[,1], reg_var[,-1] %*% t$vectors[,1])
cor(reg_var[,1], as.matrix(reg_var[,-1]) %*% as.matrix(t$vectors[,1]))
cor(reg_var)
#-------------compare the CCA result and the correlation matrix-----
#print the correlation matrix
cormat
#print the max correlation computed by CCA
R2 <- solve(R22) %*% R21 %*% R12
t <- eigen(R2)
cor(reg_var[,1], as.matrix(reg_var[,-1]) %*% as.matrix(t$vectors[,1]))
str(data_basic)
data_new <- data_basic[,6:]
data_new <- data_basic[,6:13]
summary(data_new$cbwd)
_
data.cv <- subset(data_new, cbwd = 'cv')
data.NE <- subset(data_new, cbwd = 'NE')
data.NW <- subset(data_new, cbwd = 'NW')
data.SE <- subset(data_new, cbwd = 'SE')
data.cv <- subset(data_new, cbwd = 'cv')[,-5]
data.NE <- subset(data_new, cbwd = 'NE')[,-5]
data.NW <- subset(data_new, cbwd = 'NW')[,-5]
data.SE <- subset(data_new, cbwd = 'SE')[,-5]
dim(data.cv)
centers <- matrix(0,4,7)
centers[1,] = apply(data.cv, 2, mean)
centers[2,] = apply(data.NE, 2, mean)
centers[3,] = apply(data.NW, 2, mean)
centers[4,] = apply(data.SE, 2, mean)
S <- cov(reg_var)
for (i in 1:4) {
mahal[i,] = mahalanobis(centers, centers[i,], S)
}
mahal <- matrix(0,4,4)
for (i in 1:4) {
mahal[i,] = mahalanobis(centers, centers[i,], S)
}
coords <- cmdscale(mahal)
plot(coords, type = 'n')
text(coords, labels = c('cv','NE','NW','SE'),
col = c(1,2,3,4))
mahal
centers
data_new <- data_basic[,6:13]
data.cv <- subset(data_new, cbwd = 'cv')[,-5]
data.NE <- subset(data_new, cbwd = 'NE')[,-5]
data.NW <- subset(data_new, cbwd = 'NW')[,-5]
data.SE <- subset(data_new, cbwd = 'SE')[,-5]
centers <- matrix(0,4,7)
centers[1,] = apply(data.cv, 2, mean)
centers[2,] = apply(data.NE, 2, mean)
centers[3,] = apply(data.NW, 2, mean)
centers[4,] = apply(data.SE, 2, mean)
S <- cov(reg_var)
centers
dim(data.cv)
dim(data.NE)
dim(data_new)
data_new <- data_basic[,6:13]
data.cv <- subset(data_new, cbwd == 'cv')[,-5]
data.NE <- subset(data_new, cbwd == 'NE')[,-5]
data.NW <- subset(data_new, cbwd == 'NW')[,-5]
data.SE <- subset(data_new, cbwd == 'SE')[,-5]
centers <- matrix(0,4,7)
centers[1,] = apply(data.cv, 2, mean)
centers[2,] = apply(data.NE, 2, mean)
centers[3,] = apply(data.NW, 2, mean)
centers[4,] = apply(data.SE, 2, mean)
centers
S <- cov(reg_var)
mahal <- matrix(0,4,4)
for (i in 1:4) {
mahal[i,] = mahalanobis(centers, centers[i,], S)
}
coords <- cmdscale(mahal)
plot(coords, type = 'n')
text(coords, labels = c('cv','NE','NW','SE'),
col = c(1,2,3,4))
#--------apply the MDS to the different wind direction-------
data_new <- data_basic[,6:13]
data.cv <- subset(data_new, cbwd == 'cv')[,-5]
data.NE <- subset(data_new, cbwd == 'NE')[,-5]
data.NW <- subset(data_new, cbwd == 'NW')[,-5]
data.SE <- subset(data_new, cbwd == 'SE')[,-5]
centers <- matrix(0,4,7)
centers[1,] = apply(data.cv, 2, mean)
centers[2,] = apply(data.NE, 2, mean)
centers[3,] = apply(data.NW, 2, mean)
centers[4,] = apply(data.SE, 2, mean)
S <- cov(reg_var)
mahal <- matrix(0,4,4)
for (i in 1:4) {
mahal[i,] = mahalanobis(centers, centers[i,], S)
}
coords <- cmdscale(mahal)
plot(coords, type = 'n')
text(coords, labels = c('cv','NE','NW','SE'),
col = c(1,2,3,4))
#--------apply the MDS to the different wind direction-------
data_new <- data_basic[,6:13]
data.cv <- subset(data_new, cbwd == 'cv')[,-5]
data.NE <- subset(data_new, cbwd == 'NE')[,-5]
data.NW <- subset(data_new, cbwd == 'NW')[,-5]
data.SE <- subset(data_new, cbwd == 'SE')[,-5]
centers <- matrix(0,4,7)
centers[1,] = apply(data.cv, 2, mean)
centers[2,] = apply(data.NE, 2, mean)
centers[3,] = apply(data.NW, 2, mean)
centers[4,] = apply(data.SE, 2, mean)
S <- cov(reg_var)
mahal <- matrix(0,4,4)
for (i in 1:4) {
mahal[i,] = mahalanobis(centers, centers[i,], S)
}
coords <- cmdscale(mahal)
plot(coords, type = 'n')
text(coords, labels = c('cv','NE','NW','SE'),
col = c(1,2,3,4))
